{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LeQnxjHacrU"
   },
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "049achA_acId",
    "outputId": "1161c254-2394-43b1-fe7b-6e4fdfc694c2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
       "0      5650     1.0           0     0  ...      7        1180              0   \n",
       "1      7242     2.0           0     0  ...      7        2170            400   \n",
       "2     10000     1.0           0     0  ...      6         770              0   \n",
       "3      5000     1.0           0     0  ...      7        1050            910   \n",
       "4      8080     1.0           0     0  ...      8        1680              0   \n",
       "\n",
       "   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "0      1955             0    98178  47.5112 -122.257           1340   \n",
       "1      1951          1991    98125  47.7210 -122.319           1690   \n",
       "2      1933             0    98028  47.7379 -122.233           2720   \n",
       "3      1965             0    98136  47.5208 -122.393           1360   \n",
       "4      1987             0    98074  47.6168 -122.045           1800   \n",
       "\n",
       "   sqft_lot15  \n",
       "0        5650  \n",
       "1        7639  \n",
       "2        8062  \n",
       "3        5000  \n",
       "4        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# tets\n",
    "link = \"https://raw.githubusercontent.com/murpi/wilddata/master/quests/kc_house_data.csv\"\n",
    "#dateparser = lambda x: datetime.strptime(x, \"%Y%m%dT%H%M%S\")\n",
    "df = pd.read_csv(link)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaSjnGlOZp5a"
   },
   "source": [
    "# Explore Data Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "DxZ6a7qSZpg9"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (435076705.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [49]\u001b[0;36m\u001b[0m\n\u001b[0;31m    FROM  https://realpython.com/pandas-merge-join-and-concat/\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Info\n",
    "# Describe\n",
    "# Duplicate\n",
    "\n",
    "# Fill nan (at least 10%)\n",
    "# Drop or not duplicate / Nan value\n",
    "\n",
    "# Merge or join (make sure we have enough data)\n",
    "\n",
    "DataFrame.merge(right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False\n",
    ", suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)\n",
    "\n",
    "#outer join\n",
    "EXAMPLE: outer_merged = pd.merge(precip_one_station, climate_temp, how=\"outer\", on=[\"STATION\", \"DATE\"] )\n",
    "    \n",
    "  FROM  https://realpython.com/pandas-merge-join-and-concat/\n",
    "    \n",
    "# Correlation (heatmap)\n",
    "\n",
    "DataFrame.corr(method='pearson', min_periods=1)\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html\n",
    "    \n",
    "EXAMPLE:  x.corr(y, method='spearman')  \n",
    "\n",
    " x.corr(y, method='kendall')\n",
    "\n",
    "FROM https://realpython.com/numpy-scipy-pandas-correlation-python/\n",
    "    \n",
    "HEATMAP \n",
    "\n",
    "seaborn.heatmap(data, *, vmin=None, vmax=None, cmap=None, \n",
    "                center=None, \n",
    "                robust=False, annot=None, \n",
    "                fmt='.2g', \n",
    "                annot_kws=None, linewidths=0, linecolor='white', cbar=True, cbar_kws=None, \n",
    "                cbar_ax=None, square=False, xticklabels='auto', yticklabels='auto', \n",
    "                mask=None, ax=None, **kwargs)\n",
    "\n",
    "\n",
    "    \n",
    "    EXAMPLE: Plot a heatmap for data centered on 0 with a diverging colormap:\n",
    "\n",
    "normal_data = np.random.randn(10, 12)\n",
    "ax = sns.heatmap(normal_data, center=0)\n",
    "\n",
    "FROM https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
    "    \n",
    "    \n",
    "# pairplot\n",
    "\n",
    "\n",
    "    \n",
    "    seaborn.pairplot(data, *, hue=None, hue_order=None, palette=None, vars=None, x_vars=None, \n",
    "                     y_vars=None, kind='scatter', diag_kind='auto'\n",
    "                     , markers=None, height=2.5, aspect=1, corner=False, dropna=False, plot_kws=None, \n",
    "                     diag_kws=None, grid_kws=None, size=None)\n",
    "    \n",
    "EXAMPLE:The kind parameter determines both the diagonal and off-diagonal plotting style. \n",
    "    Several options are available, including using kdeplot() to draw KDEs:\n",
    "\n",
    "sns.pairplot(penguins, kind=\"kde\")\n",
    "\n",
    "    FROM https://seaborn.pydata.org/generated/seaborn.pairplot.html\n",
    "        \n",
    "        \n",
    "\n",
    "# Drop useless cols\n",
    "\n",
    "# update columns ex dealing categorical columns ( factorisarion and dummies )\n",
    "# https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf\n",
    "# Pivot table / Group by as example\n",
    "# explode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiHx0azGZtt9"
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "SM0Bn5ZWZzSS"
   },
   "outputs": [],
   "source": [
    "# Joao -> create a system loop to test the algorithms\n",
    "\n",
    "# Logistic regression -> FranÃ§ois\n",
    "# Logistic regression + standard deviation ? -> FranÃ§ois\n",
    "# SVM -> Joao\n",
    "# Decision tree\n",
    "# KNNeighbours\n",
    "# pipeline -> joana -> for KNN & decision tree \n",
    "# pipeline -> Fabien -> for SVM\n",
    "\n",
    "# Grid search ?\n",
    "# Confusion matrix ?\n",
    "# Cross validation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification methods\n",
    "##Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "## K neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "## Decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "## Support vector machine\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data base to test\n",
    "import seaborn as sns\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic['survived'] = titanic['survived'].apply(lambda x: 'did not survive' if x == 0 else 'survived')\n",
    "titanic.dropna(inplace=True)\n",
    "# I select only the number to set the features in the following code\n",
    "X = titanic.select_dtypes(\"number\")\n",
    "\n",
    "# then the target\n",
    "y = titanic[\"survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = [{'alg': KNeighborsClassifier(), \n",
    "           'param' : {'n_neighbors' : list(range(2,20)), 'weights': ['uniform', 'distance'], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}},\n",
    "          {'alg': DecisionTreeClassifier(),\n",
    "           'param': {'max_depth': list(range(1, 20)), 'min_samples_leaf': list(range(1, 20)), 'criterion': [\"gini\", \"entropy\"]}},          \n",
    "          #insert new model \n",
    "          {'alg': Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())]),'param': {'knn__n_neighbors' : list(range(2,20))}},\n",
    "          {'alg': Pipeline([('scaler', StandardScaler()), ('dtc', DecisionTreeClassifier())]), 'param': {'dtc__max_depth' : list(range(1,20)), 'dtc__min_samples_leaf': list(range(1,20))}},\n",
    "          {'alg': Pipeline([('scaler', StandardScaler()), ('svm', svm.SVC())]), 'param': {\"svm__kernel\":[\"linear\", \"poly\", \"rbf\", \"sigmoid\"], \"svm__degree\":list(range(1,20)), \"svm__gamma\":[\"scale\", \"auto\"], \"svm__decision_function_shape\":[\"ovo\", \"ovr\"] }},\n",
    "          {\"alg\": LogisticRegression(), \"param\": {\"random_state\": list(range(2,10)), \"max_iter\": list(range(10000, 10010)), \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]} }\n",
    "         ]   \n",
    "# overall ml project\n",
    "ml = {'metric': 'accuracy', 'models': models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alg': LogisticRegression(),\n",
       " 'param': {'random_state': [2, 3, 4, 5, 6, 7, 8, 9],\n",
       "  'max_iter': [10000,\n",
       "   10001,\n",
       "   10002,\n",
       "   10003,\n",
       "   10004,\n",
       "   10005,\n",
       "   10006,\n",
       "   10007,\n",
       "   10008,\n",
       "   10009],\n",
       "  'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']},\n",
       " 'accuracy': 0.703003003003003,\n",
       " 'best_param': {'solver': 'lbfgs', 'random_state': 6, 'max_iter': 10000}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for model in ml['models']:\n",
    "    # model is a dictionary\n",
    "    rando = RandomizedSearchCV(model['alg'], model['param'], scoring = ml['metric'])\n",
    "    rando.fit(X,y)\n",
    "    model['accuracy'] = rando.best_score_\n",
    "    model['best_param'] = rando.best_params_\n",
    "\n",
    "# We can get the best model by accuracy from the loop\n",
    "ml['models'].sort(key=lambda x: x[\"accuracy\"], reverse=True)\n",
    "ml['models'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkgO-pthgxb1"
   },
   "source": [
    "We can keep grid search but mandatory for decision tree\n",
    "\n",
    "Joao works first on the algorithm then add the other works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OF895aRZwIv"
   },
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNiaig-gZ0L3"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from ast import literal_eval\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library to import text inb html\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read the HTML from the URL\n",
    "url = \"https://en.wikipedia.org/wiki/Deep-water_soloing\"\n",
    "html = request.urlopen(url).read()\n",
    "\n",
    "#regex = r\"[>](.*?)[<\\/]\"\n",
    "\n",
    "# Get text (clean html) using BeautifulSoup get_text method\n",
    "sentences = BeautifulSoup(html).get_text()\n",
    "#print(sentence)\n",
    "# Tokenize or get words\n",
    "tokens = nltk.word_tokenize(sentences)\n",
    "#df = pd.DataFrame(tokens, columns=[\"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to change string array \n",
    "get_tokens_list = lambda x: literal_eval(x) if type(x) == str else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this list allow us to set which columns to upload\n",
    "#col_list = ['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('popular')\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\") + ['\"', \"'\", \"'s\", \"-\", \"_\",\",\",\"--\",\".\", \"of\", \"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nlp = df[col_list].copy()\n",
    "# data frame to test\n",
    "df_nlp = pd.DataFrame(tokens, columns=[\"words\"])\n",
    "df_nlp.fillna(\" \", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get freq words from critics consensus\n",
    "df_nlp[\"X\"] = df_nlp[\"words\"].apply(lambda sentence: [w.lower() for w in nltk.word_tokenize(sentence)])\n",
    "freq = nltk.FreqDist(df_nlp[\"X\"].sum())\n",
    "# Data Frame word list\n",
    "df_tokens = pd.DataFrame.from_dict(freq, orient='index')\n",
    "df_tokens.columns = ['Frequency']\n",
    "df_tokens.index.name = 'Term'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get freq clean words from critics consensus\n",
    "df_nlp[\"X_clean\"] = df_nlp[\"X\"].apply(lambda tokens: [ w for w in tokens if w.lower() not in stop_words ])\n",
    "freq_clean = nltk.FreqDist(df_nlp[\"X_clean\"].sum())\n",
    "# Data Frame clean word list\n",
    "df_words = pd.DataFrame.from_dict(freq_clean, orient='index')\n",
    "df_words.columns = ['Frequency']\n",
    "df_words.index.name = 'Term'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "df_nlp[\"X_stem\"] = df_nlp[\"X_clean\"].apply(lambda words: [porter.stem(word.lower()) for word in words] )\n",
    "# change to dataFrame to see better final results\n",
    "df_stem = pd.DataFrame.from_dict(nltk.FreqDist(df_nlp[\"X_stem\"].sum()), orient='index')\n",
    "df_stem.columns = ['Frequency']\n",
    "df_stem.index.name = 'Term'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "df_nlp[\"X_lem\"] = df_nlp[\"X_clean\"].apply(lambda words: [wnl.lemmatize(word.lower()) for word in words] )\n",
    "df_lem = pd.DataFrame.from_dict(nltk.FreqDist(df_nlp[\"X_lem\"].sum()), orient='index')\n",
    "df_lem.columns = ['Frequency']\n",
    "df_lem.index.name = 'Term'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_freq = [\n",
    "    {\"X\": df_nlp[\"X\"], \"title\": \"tokens\"},\n",
    "    {\"X\": df_nlp[\"X_clean\"], \"title\": \"words\"},\n",
    "    {\"X\": df_nlp[\"X_stem\"], \"title\": \"PorterStemmer\"},\n",
    "    {\"X\": df_nlp[\"X_lem\"], \"title\": \"WordNetLemmatizer\"}    \n",
    "]\n",
    "\n",
    "# Create a matrix with vectorise of any alpha word longer than 2 caracters\n",
    "def get_count(X, name = \"\", max_features=20, stop_words=stop_words):\n",
    "    vectorizer = CountVectorizer(max_features=max_features, stop_words=stop_words )\n",
    "    text_matrix = vectorizer.fit_transform(X.apply(lambda wlist: \" \".join(wlist)))\n",
    "    df_matrix_neg = pd.DataFrame(text_matrix.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "    df_sum_matrix_neg = pd.DataFrame(df_matrix_neg.sum().sort_values(ascending=False), columns = [\"count\"])\n",
    "    \n",
    "    fig = plt.subplots(figsize=(13,7))\n",
    "    ax1 = plt.subplot(2,2,1)\n",
    "    ax2 = plt.subplot(2,2,2)\n",
    "    ax1.set(title=f\"Count words for {name}\")\n",
    "    df_sum_matrix_neg.plot(kind=\"bar\", ax = ax1)\n",
    "    #initial text\n",
    "    wordcloud_neg = WordCloud(width=300, height=300, max_font_size=200, min_font_size=10)\n",
    "    wordcloud_neg.generate_from_frequencies(df_matrix_neg.sum())\n",
    "    ax2.imshow(wordcloud_neg, interpolation=\"bilinear\")\n",
    "    ax2.axis('off')\n",
    "    ax2.set(title=f\"{name}'s wordcloud\")\n",
    "    plt.margins(x=0, y=0)\n",
    "\n",
    "for freq in list_freq:\n",
    "    get_count(freq[\"X\"], name = freq[\"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9b58IwMZyFO"
   },
   "source": [
    "# Linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "toON60D5Z7Ra"
   },
   "outputs": [],
   "source": [
    "# FranÃ§ois\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPqIBjmKfvaG"
   },
   "source": [
    "# Unsupervise algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrL8_Titf3sJ"
   },
   "source": [
    "## KNearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-bn85-tkZoB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvGCXdJfaRXQ"
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_XJlhzTaRC8"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_iris.select_dtypes(\"numbers\")\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = range(2, 10)\n",
    "inertias = []\n",
    "for k in K:\n",
    "    # Building and fitting the model\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(X)\n",
    "    kmeanModel.fit(X)\n",
    "    inertias.append(kmeanModel.inertia_)\n",
    " \n",
    "plt.plot(K, inertias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette = []\n",
    "for k in K:\n",
    "    modelKM = KMeans(n_clusters=k)\n",
    "    modelKM.fit(X)\n",
    "    silhouette.append(round(metrics.silhouette_score(X, modelKM.labels_), 4))\n",
    "\n",
    "plt.plot(K, silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxUlzKOPgCz4"
   },
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sd11-tzugB6I"
   },
   "outputs": [],
   "source": [
    "# Fabien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hackathon_preparation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
